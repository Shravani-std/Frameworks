{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "torch.compile example that demonstrates how to use torch.compile for inference"
      ],
      "metadata": {
        "id": "dK6xGz8bJlpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "def newfn(x):\n",
        "  a = torch.cos(x)\n",
        "  b = torch.sin(a)\n",
        "  return b\n",
        "\n",
        "new_fn = torch.compile(newfn, backend=\"inductor\")\n",
        "input_tensor = torch.randn(10000)\n",
        "a = new_fn(input_tensor)\n",
        "print(len(a))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WB6GjdFfI-X7",
        "outputId": "13877136-7e9a-496d-c41f-22c746f5d3d4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.compiler.list_backends()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pn73DBy1QsRv",
        "outputId": "dd84b122-1054-49ca-a208-c6af804cfac9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cudagraphs', 'inductor', 'openxla', 'tvm']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.2.0 torchvision==0.17.0"
      ],
      "metadata": {
        "id": "ECflA83oRTd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example: optimizing model"
      ],
      "metadata": {
        "id": "W9k-Zm0hRmlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "model = torch.hub.load('pytorch/vision', 'resnet50', pretrained=True)\n",
        "opt_model = torch.compile(model, backend=\"inductor\")\n",
        "opt_model(torch.randn(1,3,64,64))"
      ],
      "metadata": {
        "id": "5p4GN1TLQYXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example: Optimizing pretrained model"
      ],
      "metadata": {
        "id": "mUyNPTBdRuSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "model = torch.compile(model,backend=\"inductor\")\n",
        "\n",
        "text = \"Hello Shravani How Are you?\"\n",
        "encode_input = tokenizer(text, return_tensors='pt')\n",
        "output = model(**encode_input)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "ez2oHtZvRzq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Dynamo Tracing\n",
        "It generates computation graph"
      ],
      "metadata": {
        "id": "TQKsX3EUq_xi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîç What This Does Internally\n",
        "Dynamo intercepts execution of f(x)\n",
        "\n",
        "Wraps input into ProxyTensor\n",
        "\n",
        "Symbolically executes bytecode\n",
        "\n",
        "Builds an FX graph\n",
        "\n",
        "Returns an explanation object"
      ],
      "metadata": {
        "id": "fLxPAlFZsRVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch._dynamo as dynamo\n",
        "\n",
        "def f(x):\n",
        "    y = torch.cos(x)\n",
        "    z = torch.sin(y)\n",
        "    return z\n",
        "\n",
        "x = torch.randn(5)\n",
        "\n",
        "explanation = dynamo.explain(f)(x)\n",
        "gm = explanation.graphs[0]   # GraphModule\n",
        "\n",
        "print(\"Graph Structure:\")\n",
        "for node in gm.graph.nodes:\n",
        "    print(f\"{node.op:15} | {node.name:10} | target={node.target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DT-IFCK7rDn1",
        "outputId": "3d6f93cc-e286-490f-b0f0-6276022b3b3b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graph Structure:\n",
            "placeholder     | l_x_       | target=L_x_\n",
            "call_function   | y          | target=<built-in method cos of type object at 0x7da47b2e4b40>\n",
            "call_function   | z          | target=<built-in method sin of type object at 0x7da47b2e4b40>\n",
            "output          | output     | target=output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph Break\n",
        "Graph breaks split your program into multiple smaller compiled pieces, reducing optimization and sometimes slowing down torch.compile."
      ],
      "metadata": {
        "id": "QkbWV7fis_-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch._dynamo as dynamo\n",
        "\n",
        "def f(x):\n",
        "    y = x * 2\n",
        "    torch.save(y, \"temp.pt\")  # Unsupported operation\n",
        "    z = torch.sin(y)\n",
        "    return z\n",
        "\n",
        "x = torch.randn(4)\n",
        "\n",
        "explanation = dynamo.explain(f, x)\n",
        "\n",
        "print(\"Number of FX graphs:\", len(explanation.graphs))\n",
        "\n",
        "print(\"\\nFX Graph 1:\")\n",
        "print(explanation.graphs[0])\n",
        "\n",
        "print(\"\\nFX Graph 2:\")\n",
        "print(explanation.graphs[1])\n",
        "\n",
        "print(\"\\nGraph Break Reasons:\")\n",
        "for reason in explanation.break_reasons:\n",
        "    print(reason)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKU8MBM8skR5",
        "outputId": "ff76e732-9e71-4f57-ad0e-3a9269b56600"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of FX graphs: 2\n",
            "\n",
            "FX Graph 1:\n",
            "GraphModule()\n",
            "\n",
            "\n",
            "\n",
            "def forward(self, L_x_ : torch.Tensor):\n",
            "    l_x_ = L_x_\n",
            "    y = l_x_ * 2;  l_x_ = None\n",
            "    return (y,)\n",
            "    \n",
            "# To see more debug info, please use `graph_module.print_readable()`\n",
            "\n",
            "FX Graph 2:\n",
            "GraphModule()\n",
            "\n",
            "\n",
            "\n",
            "def forward(self, L_y_ : torch.Tensor):\n",
            "    l_y_ = L_y_\n",
            "    z = torch.sin(l_y_);  l_y_ = None\n",
            "    return (z,)\n",
            "    \n",
            "# To see more debug info, please use `graph_module.print_readable()`\n",
            "\n",
            "Graph Break Reasons:\n",
            "GraphCompileReason(reason='Attempted to call function marked as skipped\\n  Explanation: Dynamo developers have intentionally marked that the function `save` in file `/usr/local/lib/python3.12/dist-packages/torch/serialization.py` should not be traced.\\n  Hint: Avoid calling the function `save`.\\n  Hint: Apply `@torch._dynamo.dont_skip_tracing` to the function `save` to force tracing into the function. More graph breaks may occur as a result of attempting to trace into the function.\\n  Hint: Please file an issue to PyTorch.\\n\\n  Developer debug context: module: torch.serialization, qualname: save, skip reason: <missing reason>\\n\\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0007.html', user_stack=[<FrameSummary file /tmp/ipython-input-391/1972350207.py, line 6 in f>], graph_break=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Guards\n",
        "we generate conditions, which are runtime checks for these assumptions"
      ],
      "metadata": {
        "id": "TJuTUfgqttbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch._dynamo as dynamo\n",
        "\n",
        "def f(x):\n",
        "    y = x * 2\n",
        "    z = torch.sin(y)\n",
        "    return z\n",
        "\n",
        "x = torch.randn(4)\n",
        "\n",
        "explanation = dynamo.explain(f)(x)\n",
        "\n",
        "print(\"Number of FX graphs:\", len(explanation.graphs))\n",
        "\n",
        "print(\"\\nFX Graph:\")\n",
        "print(explanation.graphs[0].graph)\n",
        "\n",
        "print(\"\\nOut Guards:\")\n",
        "for guard in explanation.out_guards:\n",
        "    print(guard)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsOlDiOEt_OS",
        "outputId": "88b544ca-d9e0-46fa-d4c4-2420d4401d6f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of FX graphs: 1\n",
            "\n",
            "FX Graph:\n",
            "graph():\n",
            "    %l_x_ : torch.Tensor [num_users=1] = placeholder[target=L_x_]\n",
            "    %y : [num_users=1] = call_function[target=operator.mul](args = (%l_x_, 2), kwargs = {})\n",
            "    %z : [num_users=1] = call_function[target=torch.sin](args = (%y,), kwargs = {})\n",
            "    return (z,)\n",
            "\n",
            "Out Guards:\n",
            "Name: ''\n",
            "    Source: shape_env\n",
            "    Create Function: SHAPE_ENV\n",
            "    Guard Types: None\n",
            "    Code List: None\n",
            "    Object Weakref: None\n",
            "    Guarded Class Weakref: None\n",
            "\n",
            "Name: ''\n",
            "    Source: global\n",
            "    Create Function: DETERMINISTIC_ALGORITHMS\n",
            "    Guard Types: None\n",
            "    Code List: None\n",
            "    Object Weakref: None\n",
            "    Guarded Class Weakref: None\n",
            "\n",
            "Name: ''\n",
            "    Source: global\n",
            "    Create Function: GRAD_MODE\n",
            "    Guard Types: None\n",
            "    Code List: None\n",
            "    Object Weakref: None\n",
            "    Guarded Class Weakref: None\n",
            "\n",
            "Name: ''\n",
            "    Source: global\n",
            "    Create Function: DEFAULT_DEVICE\n",
            "    Guard Types: ['DEFAULT_DEVICE']\n",
            "    Code List: ['utils_device.CURRENT_DEVICE == None']\n",
            "    Object Weakref: None\n",
            "    Guarded Class Weakref: None\n",
            "\n",
            "Name: ''\n",
            "    Source: global\n",
            "    Create Function: GLOBAL_STATE\n",
            "    Guard Types: None\n",
            "    Code List: None\n",
            "    Object Weakref: None\n",
            "    Guarded Class Weakref: None\n",
            "\n",
            "Name: ''\n",
            "    Source: global\n",
            "    Create Function: TORCH_FUNCTION_STATE\n",
            "    Guard Types: None\n",
            "    Code List: None\n",
            "    Object Weakref: None\n",
            "    Guarded Class Weakref: None\n",
            "\n",
            "Name: ''\n",
            "    Source: global\n",
            "    Create Function: AUTOGRAD_SAVED_TENSORS_HOOKS\n",
            "    Guard Types: ['AUTOGRAD_SAVED_TENSORS_HOOKS']\n",
            "    Code List: ['torch._functorch.aot_autograd.utils.top_saved_tensors_hooks ids == None']\n",
            "    Object Weakref: None\n",
            "    Guarded Class Weakref: None\n",
            "\n",
            "Name: \"L['x']\"\n",
            "    Source: local\n",
            "    Create Function: TENSOR_MATCH\n",
            "    Guard Types: ['TENSOR_MATCH']\n",
            "    Code List: [\"hasattr(L['x'], '_dynamo_dynamic_indices') == False\"]\n",
            "    Object Weakref: <weakref at 0x7da409fe5f30; to 'Tensor' at 0x7da3fbc40f00>\n",
            "    Guarded Class Weakref: <weakref at 0x7da549f9b650; to 'torch._C._TensorMeta' at 0x118348e0 (Tensor)>\n",
            "\n",
            "Name: \"G['torch']\"\n",
            "    Source: global\n",
            "    Create Function: MODULE_MATCH\n",
            "    Guard Types: ['ID_MATCH']\n",
            "    Code List: [\"___check_obj_id(G['torch'], 138148895006192), type=<module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\"]\n",
            "    Object Weakref: <weakref at 0x7da41812abb0; to 'module' at 0x7da54bd01df0>\n",
            "    Guarded Class Weakref: <weakref at 0x7da56d6f3010; to 'type' at 0xa15a00 (module)>\n",
            "\n",
            "Name: \"G['torch'].sin\"\n",
            "    Source: global\n",
            "    Create Function: BUILTIN_MATCH\n",
            "    Guard Types: ['ID_MATCH']\n",
            "    Code List: [\"___check_obj_id(G['torch'].sin, 138148873890256), type=<built-in method sin of type object at 0x7da47b2e4b40>\"]\n",
            "    Object Weakref: <weakref at 0x7da40a127290; to 'builtin_function_or_method' at 0x7da54a8de9d0>\n",
            "    Guarded Class Weakref: <weakref at 0x7da56d6c04a0; to 'type' at 0xa15ee0 (builtin_function_or_method)>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recompilations"
      ],
      "metadata": {
        "id": "aTSQxFeVwSCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "@torch.compile\n",
        "def fn(x):\n",
        "    return x * 2\n",
        "# First call (float32)\n",
        "print(fn(torch.ones(3, 3, dtype=torch.float32)))\n",
        "\n",
        "# Second call (float64)\n",
        "print(fn(torch.ones(3, 3, dtype=torch.float64)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxgoS4UPwxDd",
        "outputId": "b4c1c002-fdef-4527-c6f5-1e4fe90377a6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2., 2., 2.],\n",
            "        [2., 2., 2.],\n",
            "        [2., 2., 2.]])\n",
            "tensor([[2., 2., 2.],\n",
            "        [2., 2., 2.],\n",
            "        [2., 2., 2.]], dtype=torch.float64)\n"
          ]
        }
      ]
    }
  ]
}